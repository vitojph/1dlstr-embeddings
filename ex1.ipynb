{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documentary-rebecca",
   "metadata": {},
   "source": [
    "# Similitud semántica\n",
    "\n",
    "Vamos a ilustrar en qué consiste la similitud semántica con un pequeño ejemplo de buscador que utiliza embeddings.\n",
    "\n",
    "Para ello, vamos a inspirarnos es este [ejemplo de buscador semántico en 3 minutos](https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes), del proyecto [`bert-as-service`](https://github.com/hanxiao/bert-as-service) de [Han Xiao](https://hanxiao.io/), y simular que estamos construyendo un motor de búsqueda semántico sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-cooling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from typing import List\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-court",
   "metadata": {},
   "source": [
    "Inventamos unos cuantos documentos (en este caso, simples oraciones) de ejemplo, con poco solapamiento entre ellas. Construimos los embeddings de las oraciones utilizando los vectores de GloVe incluidos en el modelo de inglés disponible con spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-knife",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I hate tennis.\",\n",
    "    \"I love ice cream\",\n",
    "    \"I love riding my bicycle\",\n",
    "    \"My car is a blue Hyunday\",\n",
    "    \"This session is about natural language processing and artificial intelligence\"\n",
    "]\n",
    "\n",
    "doc_vecs = [nlp(doc).vector for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-guess",
   "metadata": {},
   "source": [
    "Definimos una función que toma como argumentos de entrada un texto como query, el número de resultados que queremos mostrar, una colección de documentos en formato texto, y la misma colección de documentos vectorizada.\n",
    "\n",
    "La función procesa la query de entrada con el modelo de spaCy, construye el embedding y calcula el producto escalar con respecto a todos y cada uno de los embeddings de la colección de documentos. Después, imprime los resultados más similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-wallet",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_similar_query(\n",
    "    text: str, \n",
    "    topk: int,\n",
    "    docs: List[str], \n",
    "    doc_vecs: List[np.ndarray]\n",
    "):\n",
    "    \"\"\"Search for similar vectors.\n",
    "    It computes the dot product between a vector and a set of vectors\"\"\"\n",
    "    query = nlp(text)\n",
    "    # normalized dot product as score\n",
    "    score = np.sum(query.vector * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1)\n",
    "    topk_idx = np.argsort(score)[::-1][:topk]\n",
    "    for idx in topk_idx:\n",
    "        print(f\"[{score[idx]}]: {docs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-labor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_similar_query(\"I don't like racket sports.\", topk=3, docs=docs, doc_vecs=doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-sleep",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_similar_query(\"I own a Toyota vehicle.\", topk=3, docs=docs, doc_vecs=doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-difficulty",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_similar_query(\"I enjoy desserts.\", topk=3, docs=docs, doc_vecs=doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-dynamics",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_similar_query(\"We are talking about deep learning.\", topk=3, docs=docs, doc_vecs=doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-scoop",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_similar_query(\"I have too many bikes\", topk=3, docs=docs, doc_vecs=doc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-multiple",
   "metadata": {},
   "source": [
    "## Corpus de Chatbots\n",
    "\n",
    "A continuación vamos a repetir el proceso con una colección de mensajes más grande. En el directorio `data/` del repo he incluido un par de ficheros CSV que continene datos de entrenamiento para un chatbot en inglés y en español. Vamos a cargar los datos del dataset en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-basics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
